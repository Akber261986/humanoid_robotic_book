"use strict";(globalThis.webpackChunkhumanoid_robotic_book=globalThis.webpackChunkhumanoid_robotic_book||[]).push([[134],{2043:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var s=i(4848),r=i(8453);const o={},t="AI Paradigms in Robotics",l={id:"module1/ai_paradigms",title:"AI Paradigms in Robotics",description:"Overview",source:"@site/docs/module1/ai_paradigms.md",sourceDirName:"module1",slug:"/module1/ai_paradigms",permalink:"/humanoid_robotic_book/docs/module1/ai_paradigms",draft:!1,unlisted:!1,editUrl:"https://github.com/Akber261986/humanoid_robotic_book/edit/main/docs/module1/ai_paradigms.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Humanoid Robotics Fundamentals",permalink:"/humanoid_robotic_book/docs/module1/humanoid_fundamentals"},next:{title:"ROS 2 Setup for Robotics",permalink:"/humanoid_robotic_book/docs/module2/ros2_setup"}},a={},c=[{value:"Overview",id:"overview",level:2},{value:"Traditional AI Approaches in Robotics",id:"traditional-ai-approaches-in-robotics",level:2},{value:"Symbolic AI and Planning",id:"symbolic-ai-and-planning",level:3},{value:"Rule-Based Systems",id:"rule-based-systems",level:3},{value:"Machine Learning in Robotics",id:"machine-learning-in-robotics",level:2},{value:"Supervised Learning",id:"supervised-learning",level:3},{value:"Unsupervised Learning",id:"unsupervised-learning",level:3},{value:"Reinforcement Learning",id:"reinforcement-learning",level:3},{value:"Embodied AI and Situated Cognition",id:"embodied-ai-and-situated-cognition",level:2},{value:"Embodied Cognition Principles",id:"embodied-cognition-principles",level:3},{value:"Situated Action",id:"situated-action",level:3},{value:"Deep Learning in Robotics",id:"deep-learning-in-robotics",level:2},{value:"Convolutional Neural Networks (CNNs)",id:"convolutional-neural-networks-cnns",level:3},{value:"Recurrent Neural Networks (RNNs)",id:"recurrent-neural-networks-rnns",level:3},{value:"Transformer Architectures",id:"transformer-architectures",level:3},{value:"Learning from Demonstration",id:"learning-from-demonstration",level:2},{value:"Imitation Learning",id:"imitation-learning",level:3},{value:"Programming by Demonstration",id:"programming-by-demonstration",level:3},{value:"Integration Challenges",id:"integration-challenges",level:2},{value:"Real-Time Requirements",id:"real-time-requirements",level:3},{value:"Uncertainty and Robustness",id:"uncertainty-and-robustness",level:3},{value:"Safety and Reliability",id:"safety-and-reliability",level:3},{value:"Current Trends and Future Directions",id:"current-trends-and-future-directions",level:2},{value:"Neuromorphic Computing",id:"neuromorphic-computing",level:3},{value:"Multimodal AI",id:"multimodal-ai",level:3},{value:"Social AI",id:"social-ai",level:3},{value:"Summary",id:"summary",level:2},{value:"Example: Perception-Action Loop in Physical AI",id:"example-perception-action-loop-in-physical-ai",level:2},{value:"Verification Steps",id:"verification-steps",level:2},{value:"References",id:"references",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h1,{id:"ai-paradigms-in-robotics",children:"AI Paradigms in Robotics"}),"\n",(0,s.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(e.p,{children:"This module explores the various AI paradigms that are applied in robotics, with a focus on how they enable intelligent behavior in physical systems. We'll examine different approaches to robot intelligence and their applications in humanoid robotics."}),"\n",(0,s.jsx)(e.h2,{id:"traditional-ai-approaches-in-robotics",children:"Traditional AI Approaches in Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"symbolic-ai-and-planning",children:"Symbolic AI and Planning"}),"\n",(0,s.jsx)(e.p,{children:"Symbolic AI approaches rely on explicit representations of knowledge and logical reasoning:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Classical planning"}),": Using symbolic representations to plan sequences of actions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"STRIPS and PDDL"}),": Formal languages for specifying planning problems"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Hierarchical task networks"}),": Breaking complex tasks into simpler subtasks"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Advantages:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Interpretable and explainable"}),"\n",(0,s.jsx)(e.li,{children:"Guarantees of completeness for well-defined problems"}),"\n",(0,s.jsx)(e.li,{children:"Clear separation between knowledge representation and reasoning"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Limitations:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Struggles with uncertainty and incomplete information"}),"\n",(0,s.jsx)(e.li,{children:"Difficulty handling real-world complexity"}),"\n",(0,s.jsx)(e.li,{children:"Limited ability to learn from experience"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"rule-based-systems",children:"Rule-Based Systems"}),"\n",(0,s.jsx)(e.p,{children:'Rule-based systems use "if-then" rules to guide robot behavior:'}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Production systems"}),": Collections of condition-action rules"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Expert systems"}),": Encoding human expertise in specific domains"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Behavior trees"}),": Hierarchical organization of robot behaviors"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"machine-learning-in-robotics",children:"Machine Learning in Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"supervised-learning",children:"Supervised Learning"}),"\n",(0,s.jsx)(e.p,{children:"Supervised learning uses labeled training data to learn mappings from inputs to outputs:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Classification"}),": Recognizing objects, gestures, or states"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Regression"}),": Estimating continuous values like positions or velocities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Deep learning"}),": Using neural networks for complex pattern recognition"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Applications in robotics:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Object recognition and scene understanding"}),"\n",(0,s.jsx)(e.li,{children:"Gesture and speech recognition"}),"\n",(0,s.jsx)(e.li,{children:"Sensor data interpretation"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"unsupervised-learning",children:"Unsupervised Learning"}),"\n",(0,s.jsx)(e.p,{children:"Unsupervised learning finds patterns in unlabeled data:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Clustering"}),": Grouping similar experiences or sensor readings"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Dimensionality reduction"}),": Finding lower-dimensional representations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Anomaly detection"}),": Identifying unusual situations"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,s.jsx)(e.p,{children:"Reinforcement learning focuses on learning through interaction with an environment:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Markov Decision Processes (MDPs)"}),": Formal framework for sequential decision making"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Q-learning"}),": Learning action-value functions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Deep reinforcement learning"}),": Combining deep learning with RL"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Applications in robotics:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Motor skill learning"}),"\n",(0,s.jsx)(e.li,{children:"Locomotion control"}),"\n",(0,s.jsx)(e.li,{children:"Manipulation strategies"}),"\n",(0,s.jsx)(e.li,{children:"Navigation behaviors"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"embodied-ai-and-situated-cognition",children:"Embodied AI and Situated Cognition"}),"\n",(0,s.jsx)(e.h3,{id:"embodied-cognition-principles",children:"Embodied Cognition Principles"}),"\n",(0,s.jsx)(e.p,{children:"Embodied cognition emphasizes that cognition is shaped by the body and its interactions:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Morphological computation"}),": Physical properties that reduce computational load"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Environmental affordances"}),": Action possibilities provided by the environment"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Coupling"}),": Tight integration between perception, action, and environment"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"situated-action",children:"Situated Action"}),"\n",(0,s.jsx)(e.p,{children:"Situated action emphasizes real-time interaction with the environment:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reactive systems"}),": Direct mapping from sensors to actions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Subsumption architecture"}),": Layered control systems"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Behavior-based robotics"}),": Decomposing complex behavior into simpler behaviors"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"deep-learning-in-robotics",children:"Deep Learning in Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"convolutional-neural-networks-cnns",children:"Convolutional Neural Networks (CNNs)"}),"\n",(0,s.jsx)(e.p,{children:"CNNs are particularly effective for visual processing in robotics:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Object detection"}),": Identifying and localizing objects in images"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Semantic segmentation"}),": Labeling each pixel in an image"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Pose estimation"}),": Determining the position and orientation of objects"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"recurrent-neural-networks-rnns",children:"Recurrent Neural Networks (RNNs)"}),"\n",(0,s.jsx)(e.p,{children:"RNNs handle sequential data and temporal dependencies:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"LSTM/GRU networks"}),": Handling long-term dependencies"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sequence prediction"}),": Predicting future states or actions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language understanding"}),": Processing natural language commands"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"transformer-architectures",children:"Transformer Architectures"}),"\n",(0,s.jsx)(e.p,{children:"Transformers have revolutionized many AI applications:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Attention mechanisms"}),": Focusing on relevant information"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision transformers"}),": Alternative to CNNs for visual processing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multimodal transformers"}),": Processing multiple sensory modalities"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"learning-from-demonstration",children:"Learning from Demonstration"}),"\n",(0,s.jsx)(e.p,{children:"Learning from demonstration allows robots to acquire skills by observing humans:"}),"\n",(0,s.jsx)(e.h3,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Behavioral cloning"}),": Learning to mimic demonstrated behavior"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Inverse reinforcement learning"}),": Learning the reward function from demonstrations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Generative adversarial imitation learning"}),": Using GANs for imitation"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"programming-by-demonstration",children:"Programming by Demonstration"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Kinesthetic teaching"}),": Guiding the robot's movements physically"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Visual demonstration"}),": Teaching through video or augmented reality"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Teleoperation"}),": Remote control with subsequent autonomous execution"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"integration-challenges",children:"Integration Challenges"}),"\n",(0,s.jsx)(e.h3,{id:"real-time-requirements",children:"Real-Time Requirements"}),"\n",(0,s.jsx)(e.p,{children:"Robotic systems must operate in real-time:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Latency constraints"}),": Limited time for processing and response"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Synchronization"}),": Coordinating multiple sensors and actuators"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Resource management"}),": Efficient use of computational resources"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"uncertainty-and-robustness",children:"Uncertainty and Robustness"}),"\n",(0,s.jsx)(e.p,{children:"Real-world environments are uncertain:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensor noise"}),": Dealing with imperfect sensor readings"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Actuator errors"}),": Handling imperfections in motor control"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Environmental changes"}),": Adapting to changing conditions"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"safety-and-reliability",children:"Safety and Reliability"}),"\n",(0,s.jsx)(e.p,{children:"Robots must operate safely:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Fail-safe mechanisms"}),": Ensuring safe behavior during failures"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Validation and verification"}),": Ensuring system correctness"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Human safety"}),": Preventing harm to humans in the environment"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"current-trends-and-future-directions",children:"Current Trends and Future Directions"}),"\n",(0,s.jsx)(e.h3,{id:"neuromorphic-computing",children:"Neuromorphic Computing"}),"\n",(0,s.jsx)(e.p,{children:"Neuromorphic systems aim to mimic neural architectures:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Spiking neural networks"}),": More biologically realistic neural models"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Event-based sensors"}),": Sensors that respond to changes rather than absolute values"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Low-power computation"}),": More efficient processing for mobile robots"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"multimodal-ai",children:"Multimodal AI"}),"\n",(0,s.jsx)(e.p,{children:"Integrating multiple sensory modalities:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision-language models"}),": Understanding both visual and linguistic information"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cross-modal learning"}),": Learning from multiple sensory inputs simultaneously"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multimodal transformers"}),": Models that process multiple types of data"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"social-ai",children:"Social AI"}),"\n",(0,s.jsx)(e.p,{children:"Enabling robots to interact naturally with humans:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Social signal processing"}),": Recognizing and interpreting social cues"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Theory of mind"}),": Understanding human beliefs and intentions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Natural interaction"}),": More intuitive human-robot interfaces"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"The field of robotics draws from multiple AI paradigms, each with strengths and limitations. Modern robotic systems increasingly integrate multiple approaches, combining symbolic reasoning with learning-based methods to achieve robust, adaptive behavior. The future of robotics lies in developing more integrated, efficient, and human-compatible AI systems."}),"\n",(0,s.jsx)(e.h2,{id:"example-perception-action-loop-in-physical-ai",children:"Example: Perception-Action Loop in Physical AI"}),"\n",(0,s.jsx)(e.p,{children:"To illustrate the concepts discussed in this module, let's examine a simple implementation of a perception-action loop, which is fundamental to embodied AI systems."}),"\n",(0,s.jsx)(e.p,{children:"The perception-action loop is a continuous cycle where a robot:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Senses its environment"}),"\n",(0,s.jsx)(e.li,{children:"Processes the sensory information"}),"\n",(0,s.jsx)(e.li,{children:"Decides on an action"}),"\n",(0,s.jsx)(e.li,{children:"Acts in the environment"}),"\n",(0,s.jsx)(e.li,{children:"Repeats the cycle"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Here's a simplified Python example:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import time\nimport random\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass SensorData:\n    proximity_sensors: List[float]  # Distance readings from proximity sensors (0.0 to 1.0)\n    light_sensors: List[float]      # Light intensity readings (0.0 to 1.0)\n    sound_direction: float          # Direction of loudest sound (-1.0 to 1.0, left to right)\n\n@dataclass\nclass Action:\n    forward_speed: float           # Forward/backward speed (-1.0 to 1.0)\n    turn_direction: float          # Turning direction (-1.0 to 1.0, left to right)\n    arm_position: float            # Arm position (0.0 to 1.0)\n\nclass SimpleRobot:\n    def __init__(self, name: str):\n        self.name = name\n        self.position = [0.0, 0.0]  # x, y coordinates\n        self.orientation = 0.0      # Facing direction in radians\n        self.battery_level = 100.0  # Battery percentage\n\n    def sense(self) -> SensorData:\n        # Simulate proximity sensors (front, left, right)\n        proximity = [\n            random.uniform(0.2, 0.8),  # Front\n            random.uniform(0.1, 0.9),  # Left\n            random.uniform(0.1, 0.9)   # Right\n        ]\n\n        # Simulate light sensors (left, center, right)\n        light = [\n            random.uniform(0.0, 1.0),  # Left\n            random.uniform(0.0, 1.0),  # Center\n            random.uniform(0.0, 1.0)   # Right\n        ]\n\n        # Simulate sound direction\n        sound_direction = random.uniform(-1.0, 1.0)\n\n        return SensorData(\n            proximity_sensors=proximity,\n            light_sensors=light,\n            sound_direction=sound_direction\n        )\n\n    def process_sensory_data(self, sensor_data: SensorData) -> Action:\n        # Simple behavior: Move toward light, avoid obstacles\n        front_proximity = sensor_data.proximity_sensors[0]\n\n        if front_proximity < 0.3:  # Avoid obstacles\n            turn_direction = random.choice([-0.5, 0.5])\n            forward_speed = -0.3  # Move backward slightly\n        else:  # Move toward brighter light\n            center_light = sensor_data.light_sensors[1]\n            left_light = sensor_data.light_sensors[0]\n            right_light = sensor_data.light_sensors[2]\n\n            if center_light > max(left_light, right_light):\n                forward_speed = 0.5\n                turn_direction = 0.0\n            elif left_light > right_light:\n                forward_speed = 0.3\n                turn_direction = -0.3\n            else:\n                forward_speed = 0.3\n                turn_direction = 0.3\n\n        arm_position = abs(sensor_data.sound_direction) * 0.5 + 0.25\n\n        return Action(\n            forward_speed=forward_speed,\n            turn_direction=turn_direction,\n            arm_position=arm_position\n        )\n\n    def act(self, action: Action):\n        # Update position based on action\n        self.position[0] += action.forward_speed * 0.1\n        self.position[1] += action.turn_direction * 0.05\n        self.orientation += action.turn_direction * 0.05\n        self.battery_level -= 0.1\n\n    def perception_action_loop(self, iterations: int = 10):\n        for i in range(iterations):\n            sensor_data = self.sense()                    # PERCEPTION\n            action = self.process_sensory_data(sensor_data)  # COGNITION\n            self.act(action)                              # ACTION\n            time.sleep(0.1)  # Simulate real-time operation\n"})}),"\n",(0,s.jsx)(e.p,{children:"This example demonstrates how different AI paradigms work together in a physical system:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["The robot uses ",(0,s.jsx)(e.strong,{children:"reactive behavior"})," to avoid obstacles"]}),"\n",(0,s.jsxs)(e.li,{children:["It employs ",(0,s.jsx)(e.strong,{children:"simple planning"})," to move toward light sources"]}),"\n",(0,s.jsxs)(e.li,{children:["It integrates ",(0,s.jsx)(e.strong,{children:"multiple sensory modalities"})," (proximity, light, sound)"]}),"\n",(0,s.jsxs)(e.li,{children:["It maintains a ",(0,s.jsx)(e.strong,{children:"continuous loop"})," of perception, decision-making, and action"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"verification-steps",children:"Verification Steps"}),"\n",(0,s.jsx)(e.p,{children:"To verify the concepts in this module:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Understanding Check"}),": Can you explain the difference between symbolic AI and learning-based approaches in robotics?"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Application"}),": Can you identify which AI paradigm would be most suitable for a specific robotic task (e.g., object recognition, path planning, manipulation)?"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Implementation"}),": Can you run the provided perception-action loop example and modify it to exhibit different behaviors?"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Analysis"}),": Can you identify the perception-action loop in real-world robotic systems?"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://aima.cs.berkeley.edu/",children:"Russell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach (4th ed.). Pearson."})," - Comprehensive textbook on AI approaches"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://mitpress.mit.edu/9780262039246/reinforcement-learning/",children:"Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press."})," - Definitive text on reinforcement learning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://arxiv.org/abs/2008.09565",children:"Deep Learning in Robotics Survey"})," - Recent survey of deep learning applications in robotics (published within past 5 years as required)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://index.ros.org/packages/?q=ai",children:"ROS 2 AI Packages Documentation"})," - Official documentation for AI-related ROS 2 packages"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://www.jmlr.org/robotics/",children:"Journal of Machine Learning Research - Robotics"})," - Machine learning applications in robotics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://www.cs.cmu.edu/~illah/ROBOTICS/",children:"AI for Robotics"})," - Course materials from CMU"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://arxiv.org/abs/1901.08329",children:"Deep Learning for Robotics"})," - Survey of deep learning in robotics"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>l});var s=i(6540);const r={},o=s.createContext(r);function t(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);