{
  "permissions": {
    "allow": [
      "Bash(python -m py_compile docs/module3/examples/vla_command_parsing.py docs/module3/examples/visual_grounding.py docs/module4/examples/simulated_deployment.py)",
      "Bash(python3 -m py_compile docs/module3/examples/vla_command_parsing.py docs/module3/examples/visual_grounding.py docs/module4/examples/simulated_deployment.py)",
      "SlashCommand(/sp.constitution)",
      "SlashCommand(/sp.specify)",
      "SlashCommand(/sp.plan)",
      "Bash(chmod +x deploy.sh)",
      "Bash(pip3 install -r requirements.txt)",
      "Bash(python3 -m venv venv)",
      "Bash(source venv/bin/activate)",
      "Bash(pip install -r requirements.txt)",
      "Bash(rm src/components/floatingChat.css src/components/FloatingChatWidget.tsx src/pages/chat/chat.css src/pages/chat/index.tsx)",
      "Bash(rm -rf history/prompts/001-physical-ai-robotics-book/)",
      "Bash(rm -rf history/prompts/general/)",
      "Bash(git fetch:*)",
      "Bash(.specify/scripts/bash/create-new-feature.sh --json \"Detailed Specifications: RAG-Enabled Gemini Chatbot for Humanoid Robotics Book\nBuilding on the constitution, this spec defines functional/non-functional requirements, architecture, inputs/outputs, and edge cases. Adapted from basic Gemini chat examples (e.g., Streamlit + Gemini API flows in chatkit-gemini-bot), but layered with RAG for book-specific querying.\nFunctional Requirements\n1.    Data Ingestion (Indexing):\no    Source: Markdown files from book''s docs/ directory (e.g., chapters on humanoid kinematics, AI integration, hackathon projects).\no    Chunking: Split MD into 500-800 char semantic chunks (using simple sentence splitters; preserve headings/code blocks).\no    Embeddings: Use Gemini''s embedding-001 model to generate 768-dim vectors per chunk.\no    Storage: Upsert chunks + metadata (file_path, chunk_id, text) into local Qdrant collection \"book_vectors\".\no    Trigger: Run once via python index.py or on app startup if collection empty.\n2.    Query Handling (RAG Pipeline):\no    Input: User text query (e.g., \"Explain inverse kinematics in humanoids\").\no    Embed Query: Gemini embedding-001 on query text.\no    Retrieve: Search Qdrant for top-3 similar chunks (cosine similarity >0.7 threshold).\no    Augment: Construct prompt: \"Based on this book excerpt: {retrieved_chunks}. Answer: {query}. Cite sources.\"\no    Generate: Use Gemini 1.5 Flash for concise, educational response (<300 words).\no    Output: Response + cited chunk previews in chat UI.\n3.    Chat UI:\no    Framework: Streamlit (chat interface like chatkit-gemini-bot).\no    Features: Persistent session history, clear/reset button, loading spinner, source citations as expandable accordions.\no    Integration: Standalone app.py; suggest embedding in Docusaurus via <iframe src=\"http://localhost:8501\"> for book site.\n4.    Claude Code Integration:\no    All code generated/refined via Claude''s \"Code Skills\" (e.g., prompt Claude with specs for modules).\no    Structure: Modular Python files with docstrings; use type hints for clarity.\nNon-Functional Requirements\n•    Performance: <3s end-to-end latency; handle 100+ queries/session.\n•    Scalability: Qdrant local (Docker); auto-reindex on book updates.\n•    Dependencies: google-generativeai, qdrant-client, streamlit, markdown, python-dotenv (for GEMINI_API_KEY).\n•    Error Handling: Graceful fallbacks (e.g., \"No relevant info found\" if retrieval empty); log errors to console.\n•    Testing: Unit tests for embedding/retrieval; manual smoke test with 5 book queries.\n•    Edge Cases:\no    Empty DB: Prompt to run indexing.\no    Irrelevant Query: Redirect to book topics.\no    Long Query: Truncate to 1000 chars.\no    API Errors: Retry 3x with exponential backoff.\n•    Deploye on Railway\nInputs/Outputs\n•    Ingestion Input: Path to docs/ dir.\n•    Query Input: String (max 500 chars).\n•    Response Output: JSON-like {\"response\": str, \"sources\": list[dict{\"text\": str, \"file\": str}]}.\n•    Config: .env with GEMINI_API_KEY; Qdrant URL: http://localhost:6333.\nThis spec is exhaustive yet implementable in <500 LOC. Any deviation requires constitution approval.\" --number 1 --short-name \"rag-chatbot\")",
      "Bash(.specify/scripts/bash/check-prerequisites.sh --json)",
      "Bash(.specify/scripts/bash/create-adr.sh --help)",
      "Bash(.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks)",
      "Bash(python test_structure.py)",
      "Bash(ls -la test_*.py verify_*.py)",
      "Bash(pip install pydantic==2.9.2)",
      "Bash(pip install google-generativeai==0.8.3)",
      "Bash(python -c \"import sys; sys.path.insert(0, ''.''); from src.api.main import app; print(''Backend import successful'')\")",
      "Bash(python verify_setup.py)",
      "Bash(git push origin 001-rag-chatbot)",
      "Bash(git add backend/Dockerfile)",
      "Bash(git add backend/src/api/main.py railway.toml)"
    ]
  },
  "disabledMcpjsonServers": [
    "context7",
    "github"
  ]
}
